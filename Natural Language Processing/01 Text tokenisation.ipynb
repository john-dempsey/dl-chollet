{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZRsKDz2fhuryfuWht0LFB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import string\n","import re\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from keras.layers import TextVectorization"],"metadata":{"id":"inxx7PUVeIJK","executionInfo":{"status":"ok","timestamp":1702375997774,"user_tz":0,"elapsed":7256,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YSpmHdYad8D5","executionInfo":{"status":"ok","timestamp":1702375997774,"user_tz":0,"elapsed":10,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}}},"outputs":[],"source":["class Vectorizer:\n","    def standardize(self, text):\n","        text = text.lower()\n","        return \"\".join(char for char in text if char not in string.punctuation)\n","\n","    def tokenize(self, text):\n","        text = self.standardize(text)\n","        return text.split()\n","\n","    def make_vocabulary(self, dataset):\n","        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n","        for text in dataset:\n","            text = self.standardize(text)\n","            tokens = self.tokenize(text)\n","            for token in tokens:\n","                if token not in self.vocabulary:\n","                    self.vocabulary[token] = len(self.vocabulary)\n","        self.inverse_vocabulary = dict(\n","            (v, k) for k, v in self.vocabulary.items())\n","\n","    def encode(self, text):\n","        text = self.standardize(text)\n","        tokens = self.tokenize(text)\n","        return [self.vocabulary.get(token, 1) for token in tokens]\n","\n","    def decode(self, int_sequence):\n","        return \" \".join(\n","            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"]},{"cell_type":"code","source":["vectorizer = Vectorizer()\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","vectorizer.make_vocabulary(dataset)"],"metadata":{"id":"xy-p_jy-gaXy","executionInfo":{"status":"ok","timestamp":1702375997774,"user_tz":0,"elapsed":10,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = vectorizer.encode(test_sentence)\n","print(encoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5I7gCYWeMDv","executionInfo":{"status":"ok","timestamp":1702375997774,"user_tz":0,"elapsed":10,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}},"outputId":"1ee2fcf3-6d4d-4fcb-8eca-22b64ce037f1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 3, 5, 7, 1, 5, 6]\n"]}]},{"cell_type":"code","source":["decoded_sentence = vectorizer.decode(encoded_sentence)\n","print(decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-n-nZwWReMrq","executionInfo":{"status":"ok","timestamp":1702375997774,"user_tz":0,"elapsed":2,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}},"outputId":"fc51b0ac-6573-4a73-ffac-9eeafa234313"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["i write rewrite and [UNK] rewrite again\n"]}]},{"cell_type":"code","source":["text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n",")"],"metadata":{"id":"6v416MsvewqC","executionInfo":{"status":"ok","timestamp":1702375997774,"user_tz":0,"elapsed":2,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def custom_standardization_fn(string_tensor):\n","    lowercase_string = tf.strings.lower(string_tensor)\n","    return tf.strings.regex_replace(\n","        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n","\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)\n","\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n","    standardize=custom_standardization_fn,\n","    split=custom_split_fn,\n",")"],"metadata":{"id":"jlpcnudJe1XJ","executionInfo":{"status":"ok","timestamp":1702375997774,"user_tz":0,"elapsed":2,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["text_vectorization.adapt(dataset)"],"metadata":{"id":"uosPQOgLe6q7","executionInfo":{"status":"ok","timestamp":1702375998226,"user_tz":0,"elapsed":453,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["vocabulary = text_vectorization.get_vocabulary()\n","vocabulary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iyMcOpVne9tg","executionInfo":{"status":"ok","timestamp":1702375998226,"user_tz":0,"elapsed":3,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}},"outputId":"54254bc5-1654-4d58-a32b-3b9aa0832b9b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'erase',\n"," 'write',\n"," 'then',\n"," 'rewrite',\n"," 'poppy',\n"," 'i',\n"," 'blooms',\n"," 'and',\n"," 'again',\n"," 'a']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["encoded_sentence = text_vectorization(test_sentence)\n","print(encoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUKol8MYe-UL","executionInfo":{"status":"ok","timestamp":1702375998226,"user_tz":0,"elapsed":2,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}},"outputId":"fc0e83f0-faa0-4a28-cdc5-dc94d238b39d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"]}]},{"cell_type":"code","source":["inverse_vocab = dict(enumerate(vocabulary))\n","decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n","print(decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjSmdrLHfNJ4","executionInfo":{"status":"ok","timestamp":1702375998226,"user_tz":0,"elapsed":2,"user":{"displayName":"John Dempsey","userId":"09438879888700756457"}},"outputId":"7aa40f88-6a8e-4590-84a5-b541ab01891a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["i write rewrite and [UNK] rewrite again\n"]}]}]}